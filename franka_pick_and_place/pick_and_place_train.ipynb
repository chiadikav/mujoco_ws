{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb44978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Activate virtual environment\n",
    "venv_path = os.path.expanduser(\"~/mujoco_ws/venv\")\n",
    "if os.path.exists(venv_path):\n",
    "    sys.path.insert(0, os.path.join(venv_path, \"lib\", \"python3.11\", \"site-packages\"))\n",
    "    print(f\"✓ Virtual environment activated: {venv_path}\")\n",
    "    print(f\"✓ Python executable: {sys.executable}\")\n",
    "else:\n",
    "    print(f\"⚠ Virtual environment not found at {venv_path}\")\n",
    "    print(f\"Currently using Python: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de253074",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c49dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mujoco\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import time\n",
    "import torch\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch using: {'cuda' if torch.cuda.is_available() else 'cpu'} device\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"MuJoCo version: {mujoco.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c5d72",
   "metadata": {},
   "source": [
    "## Environment: Touch the Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickPlaceEnv(gym.Env):\n",
    "    \"\"\"Franka Panda gripper environment (inspired by panda-gym).\n",
    "    \n",
    "    Key improvements:\n",
    "    - Uses joint position control with incremental targets (like panda-gym)\n",
    "    - Limits action to 0.05 rad/step for smooth, learnable control\n",
    "    - Observation includes gripper width (not individual fingers)\n",
    "    - Uses proper neutral pose and joint limit awareness\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path='model/pick_place_scene.xml', render_mode=None):\n",
    "        super().__init__()\n",
    "        self.render_mode = render_mode\n",
    "        self.viewer = None\n",
    "        \n",
    "        # Load model\n",
    "        try:\n",
    "            self.model = mujoco.MjModel.from_xml_path(model_path)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Model not found at {model_path}\")\n",
    "        \n",
    "        self.data = mujoco.MjData(self.model)\n",
    "        self.dt = self.model.opt.timestep\n",
    "        \n",
    "        # Franka Panda configuration (7 arm joints + 2 gripper fingers)\n",
    "        self.arm_joint_indices = np.array([0, 1, 2, 3, 4, 5, 6])\n",
    "        self.gripper_joint_indices = np.array([7, 8])\n",
    "        \n",
    "        # Joint forces (from panda-gym reference)\n",
    "        self.joint_forces = np.array([87.0, 87.0, 87.0, 87.0, 12.0, 120.0, 120.0, 170.0, 170.0])\n",
    "        \n",
    "        # Neutral pose (from panda-gym)\n",
    "        self.neutral_joint_values = np.array([0.00, 0.41, 0.00, -1.85, 0.00, 2.26, 0.79, 0.04, 0.04])\n",
    "        \n",
    "        # Action space: 7 arm joints + 1 gripper (incremental control [-1, 1])\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-1.0]*8),\n",
    "            high=np.array([1.0]*8),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Observation space: EE position (3) + EE velocity (3) + gripper width (1) + box position (3) = 10D\n",
    "        # Simplified observation like panda-gym\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.max_episode_steps = 500\n",
    "        self.step_count = 0\n",
    "        self.active_box = None\n",
    "        \n",
    "        # Hand/gripper link index\n",
    "        self.hand_id = self.model.body('hand').id\n",
    "        self.max_action_step = 0.05  # Limit control to 0.05 rad per step (like panda-gym)\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"Get observation: EE position/velocity + gripper width + box position.\"\"\"\n",
    "        # End effector (hand body) position and velocity\n",
    "        ee_pos = self.data.body(self.hand_id).xpos.copy()\n",
    "        ee_vel = self.data.body(self.hand_id).cvel[:3].copy()  # Linear velocity\n",
    "        \n",
    "        # Gripper width (sum of both finger angles)\n",
    "        gripper_width = self.data.qpos[7] + self.data.qpos[8]\n",
    "        \n",
    "        # Target box position\n",
    "        box_pos = self.data.body(self.active_box).xpos.copy()\n",
    "        \n",
    "        obs = np.concatenate([\n",
    "            ee_pos, ee_vel, [gripper_width], box_pos\n",
    "        ]).astype(np.float32)\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset environment to neutral pose with random box.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "        \n",
    "        # Set to neutral pose\n",
    "        self.data.qpos[:len(self.neutral_joint_values)] = self.neutral_joint_values\n",
    "        \n",
    "        # Randomly select target box\n",
    "        all_boxes = [\n",
    "            'red_box_0', 'red_box_1', 'red_box_2',\n",
    "            'blue_box_0', 'blue_box_1', 'blue_box_2',\n",
    "            'green_box_0', 'green_box_1', 'green_box_2',\n",
    "            'yellow_box_0', 'yellow_box_1', 'yellow_box_2'\n",
    "        ]\n",
    "        self.active_box = np.random.choice(all_boxes)\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Settle physics\n",
    "        for _ in range(10):\n",
    "            mujoco.mj_step(self.model, self.data)\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "        return obs, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute action: controlled descent from above with limited velocity.\"\"\"\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        \n",
    "        # Get current state\n",
    "        ee_pos = self.data.body(self.hand_id).xpos.copy()\n",
    "        box_pos = self.data.body(self.active_box).xpos.copy()\n",
    "        \n",
    "        # Current arm angles\n",
    "        current_arm_angles = self.data.qpos[self.arm_joint_indices].copy()\n",
    "        \n",
    "        # Target: directly above box at safe height\n",
    "        approach_height = box_pos[2] + 0.10  # 10cm above box to start approach\n",
    "        target_xyz = np.array([box_pos[0], box_pos[1], approach_height])\n",
    "        \n",
    "        # Calculate errors\n",
    "        pos_error = target_xyz - ee_pos\n",
    "        horiz_error = np.linalg.norm(pos_error[:2])\n",
    "        vert_error = pos_error[2]\n",
    "        \n",
    "        # Arm control based on distance to approach position\n",
    "        if horiz_error > 0.02 or vert_error > 0.02:  # Still far from target\n",
    "            # Scale toward target with action influence\n",
    "            arm_ctrl = action[:7] * self.max_action_step * 0.4  # Reduced speed toward target\n",
    "        else:\n",
    "            # Close to target, very fine control\n",
    "            arm_ctrl = action[:7] * self.max_action_step * 0.2  # Very slow fine motion\n",
    "        \n",
    "        target_arm_angles = current_arm_angles + arm_ctrl\n",
    "        \n",
    "        # Gripper control: slower, more controlled\n",
    "        gripper_ctrl = action[7] * 0.01  # Very slow gripper (1cm per step max)\n",
    "        current_gripper_width = self.data.qpos[7] + self.data.qpos[8]\n",
    "        target_gripper_width = current_gripper_width + gripper_ctrl\n",
    "        target_gripper_width = np.clip(target_gripper_width, 0, 0.04)\n",
    "        \n",
    "        # Combine target angles\n",
    "        target_angles = np.concatenate([\n",
    "            target_arm_angles,\n",
    "            [target_gripper_width / 2, target_gripper_width / 2]\n",
    "        ])\n",
    "        \n",
    "        # Control with adaptive damping based on proximity to target\n",
    "        is_near_target = horiz_error < 0.05 and vert_error < 0.05\n",
    "        self._control_joints(target_angles, slow_descent=is_near_target)\n",
    "        \n",
    "        # More substeps for smoother motion\n",
    "        num_substeps = 15 if is_near_target else 10\n",
    "        for _ in range(num_substeps):\n",
    "            mujoco.mj_step(self.model, self.data)\n",
    "        \n",
    "        self.step_count += 1\n",
    "        obs = self._get_obs()\n",
    "        \n",
    "        # Compute reward\n",
    "        ee_pos_new = self.data.body(self.hand_id).xpos.copy()\n",
    "        box_pos = self.data.body(self.active_box).xpos.copy()\n",
    "        \n",
    "        # Reward for approaching from above (xy alignment)\n",
    "        horiz_dist = np.linalg.norm(ee_pos_new[:2] - box_pos[:2])\n",
    "        vert_dist = ee_pos_new[2] - box_pos[2]\n",
    "        \n",
    "        reward = 0.0\n",
    "        \n",
    "        # Phase 1: Horizontal alignment (xy plane)\n",
    "        if horiz_dist < 0.15:\n",
    "            reward += 30.0 * max(0, 1.0 - horiz_dist / 0.15)\n",
    "        \n",
    "        # Phase 2: Vertical approach from above\n",
    "        if horiz_dist < 0.08 and vert_dist > -0.02:  # Close horizontally and above box\n",
    "            reward += 40.0 * max(0, 1.0 - (vert_dist - 0.06) / 0.06)\n",
    "        \n",
    "        # Touch reward: gentle contact with box top\n",
    "        touch_dist = np.linalg.norm(ee_pos_new - box_pos)\n",
    "        if touch_dist < 0.10:  # Close enough to touch\n",
    "            reward += 100.0 * max(0, 1.0 - touch_dist / 0.10)\n",
    "            # Bonus for coming from above (positive z trajectory)\n",
    "            if vert_dist < 0.02:\n",
    "                reward += 50.0\n",
    "        \n",
    "        # Penalize high joint velocities to discourage slamming\n",
    "        joint_vel_penalty = -np.linalg.norm(self.data.qvel[:7]) * 0.005\n",
    "        reward += joint_vel_penalty\n",
    "        \n",
    "        # Time penalty\n",
    "        reward -= 0.0003\n",
    "        \n",
    "        terminated = self.step_count >= self.max_episode_steps\n",
    "        \n",
    "        return obs, reward, terminated, False, {}\n",
    "    \n",
    "    def _control_joints(self, target_angles, slow_descent=False):\n",
    "        \"\"\"Control joints using PD control with adaptive gains for smooth motion.\"\"\"\n",
    "        # Proportional and derivative gains (reduced during descent for smoother control)\n",
    "        kp = 80.0 if slow_descent else 100.0  # Lower gain during descent\n",
    "        kd = 15.0 if slow_descent else 10.0   # Higher damping for smoother descent\n",
    "        \n",
    "        # For arm joints (0-6): use PD control\n",
    "        for i in range(7):\n",
    "            current_angle = self.data.qpos[i]\n",
    "            error = target_angles[i] - current_angle\n",
    "            \n",
    "            ctrl = kp * error\n",
    "            \n",
    "            vel = self.data.qvel[i]\n",
    "            ctrl -= kd * vel\n",
    "            \n",
    "            # Clip to force limit\n",
    "            force_limit = self.joint_forces[i]\n",
    "            ctrl = np.clip(ctrl, -force_limit, force_limit)\n",
    "            \n",
    "            self.data.ctrl[i] = ctrl\n",
    "        \n",
    "        # For gripper (control index 7 maps to gripper tendon)\n",
    "        gripper_width_cmd = target_angles[7] * target_angles[8]\n",
    "        gripper_ctrl = int((gripper_width_cmd / 0.04) * 255)\n",
    "        gripper_ctrl = np.clip(gripper_ctrl, 0, 255)\n",
    "        self.data.ctrl[7] = gripper_ctrl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce90125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pick_place(total_timesteps=100000):\n",
    "    \"\"\"Train Franka Panda gripper to touch randomly placed boxes (improved control).\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Training: Touch the randomly placed box (panda-gym style control)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Task: Train gripper to reach and touch one randomly placed box\")\n",
    "    print(f\"Improvement: Incremental joint position control (0.05 rad/step max)\")\n",
    "    print(f\"Device: {('CUDA' if torch.cuda.is_available() else 'CPU')}\")\n",
    "    print()\n",
    "    \n",
    "    # Create environment\n",
    "    env = make_vec_env(\n",
    "        lambda: PickPlaceEnv(model_path='model/pick_place_scene.xml'),\n",
    "        n_envs=4\n",
    "    )\n",
    "    \n",
    "    # Train with PPO\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=64,\n",
    "        n_epochs=20,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1,\n",
    "        policy_kwargs={\"net_arch\": [256, 256]}\n",
    "    )\n",
    "    \n",
    "    print(f\"Training for {total_timesteps:,} timesteps...\\n\")\n",
    "    start_time = time.time()\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    model.save(\"touch_box_ppo\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"✓ Training complete! ({elapsed/60:.1f} minutes)\")\n",
    "    print(f\"✓ Model saved as 'touch_box_ppo'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    env.close()\n",
    "    return model\n",
    "\n",
    "print(\"✓ train_pick_place() function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pp = train_pick_place(total_timesteps=400000)  # Train with better approach guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(num_episodes=5, deterministic=True):\n",
    "    \"\"\"Evaluate the model - check how often it touches the box.\"\"\"\n",
    "    from stable_baselines3 import PPO\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Evaluating Touch Box Model (Improved Control)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = PPO.load(\"touch_box_ppo\")\n",
    "    \n",
    "    touches = 0\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        print(f\"\\nEpisode {ep + 1}/{num_episodes}:\")\n",
    "        \n",
    "        env = PickPlaceEnv(model_path='model/pick_place_scene.xml')\n",
    "        obs, _ = env.reset()\n",
    "        box_name = env.active_box\n",
    "        \n",
    "        episode_reward = 0.0\n",
    "        touched = False\n",
    "        \n",
    "        while env.step_count < env.max_episode_steps:\n",
    "            action, _ = model.predict(obs, deterministic=deterministic)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Get current distance from obs (indices 0:3 are EE pos, 7:10 are box pos)\n",
    "            ee_pos = obs[:3]\n",
    "            box_pos = obs[7:10]\n",
    "            distance = np.linalg.norm(ee_pos - box_pos)\n",
    "            \n",
    "            if distance < 0.20:  # Increased from 0.15m\n",
    "                touched = True\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        if touched:\n",
    "            touches += 1\n",
    "        \n",
    "        print(f\"  Box: {box_name}\")\n",
    "        print(f\"  Reward: {episode_reward:.2f}\")\n",
    "        print(f\"  Touched: {'✓ YES' if touched else '✗ NO'}\")\n",
    "        \n",
    "        env.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Success Rate: {touches}/{num_episodes} ({100*touches/num_episodes:.0f}%)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print(\"✓ evaluate_model() function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809ec74",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Current Task**: Train Franka Panda arm to touch a randomly-placed box\n",
    "- Single box per episode (randomly selected from 12 available)\n",
    "- Dense reaching reward: 50.0 × (1 - distance/0.8)\n",
    "- Touch bonus: +100.0 when distance < 0.15m\n",
    "- Time penalty: -0.001 per step\n",
    "- 500 steps per episode\n",
    "- Observation: 22D (arm state + gripper + end-effector + box position)\n",
    "- Action: 8D continuous (7 arm joints + 1 gripper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9241e9e",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(num_episodes=3, deterministic=True):\n",
    "    \"\"\"Visualize the trained model.\"\"\"\n",
    "    from stable_baselines3 import PPO\n",
    "    import mujoco.viewer\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Visualizing Touch Box Model\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Episodes: {num_episodes}\\n\")\n",
    "    \n",
    "    model = PPO.load(\"touch_box_ppo\")\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        print(f\"Episode {ep + 1}/{num_episodes}\")\n",
    "        \n",
    "        env = PickPlaceEnv(model_path='model/pick_place_scene.xml')\n",
    "        obs, _ = env.reset()\n",
    "        \n",
    "        viewer = mujoco.viewer.launch_passive(env.model, env.data)\n",
    "        \n",
    "        try:\n",
    "            while env.step_count < env.max_episode_steps:\n",
    "                action, _ = model.predict(obs, deterministic=deterministic)\n",
    "                obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "                \n",
    "                viewer.sync()\n",
    "                \n",
    "        finally:\n",
    "            viewer.close()\n",
    "            env.close()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Done!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "print(\"✓ visualize_model() function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08463dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(num_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5324472f",
   "metadata": {},
   "source": [
    "## Debugging and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f46d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_environment():\n",
    "    \"\"\"Debug the environment to understand observations and actions (improved control).\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Environment Debugging Information\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    env = PickPlaceEnv(model_path='model/pick_place_scene.xml')\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    print(f\"\\n✓ Environment initialized successfully (panda-gym style)\")\n",
    "    print(f\"  Model: Franka Panda with parallel gripper\")\n",
    "    print(f\"  Action space: {env.action_space.shape} (7 arm joints + 1 gripper)\")\n",
    "    print(f\"  Observation space: {env.observation_space.shape} (simplified)\")\n",
    "    \n",
    "    print(f\"\\nObservation breakdown (10D total):\")\n",
    "    print(f\"  [0:3]    = End-effector (hand) position (m)\")\n",
    "    print(f\"  [3:6]    = End-effector velocity (m/s)\")\n",
    "    print(f\"  [6]      = Gripper width (m, 0 to 0.04)\")\n",
    "    print(f\"  [7:10]   = Target box position (m)\")\n",
    "    \n",
    "    print(f\"\\nInitial observation sample:\")\n",
    "    print(f\"  EE position: {obs[:3]}\")\n",
    "    print(f\"  EE velocity: {obs[3:6]}\")\n",
    "    print(f\"  Gripper width: {obs[6]:.4f}\")\n",
    "    print(f\"  Box position: {obs[7:10]}\")\n",
    "    \n",
    "    # Test an action with incremental control\n",
    "    action = np.zeros(8)\n",
    "    action[0] = 0.5   # Move joint 1 (incremental, 0.025 rad max)\n",
    "    action[7] = -0.5  # Close gripper (incremental, 0.025m max)\n",
    "    \n",
    "    print(f\"\\nTest action (move joint 1 incrementally, close gripper):\")\n",
    "    print(f\"  Action: {action}\")\n",
    "    print(f\"  Max step: 0.05 rad/step for arm, 0.05m/step for gripper\")\n",
    "    \n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    print(f\"  Reward: {reward:.4f}\")\n",
    "    print(f\"  New EE position: {obs[:3]}\")\n",
    "    print(f\"  New gripper width: {obs[6]:.4f}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"✓ Environment is working correctly!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "print(\"✓ debug_environment() function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ca394",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify new observation format\n",
    "env = PickPlaceEnv(model_path='model/pick_place_scene.xml')\n",
    "obs, _ = env.reset()\n",
    "\n",
    "print(\"Observation Space Test\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Observation shape: {obs.shape} (expected (10,))\")\n",
    "print(f\"Observation dtype: {obs.dtype}\")\n",
    "print(f\"\\nObservation values:\")\n",
    "print(f\"  EE position (obs[0:3]): {obs[:3]}\")\n",
    "print(f\"  EE velocity (obs[3:6]): {obs[3:6]}\")\n",
    "print(f\"  Gripper width (obs[6]): {obs[6]:.4f}\")\n",
    "print(f\"  Box position (obs[7:10]): {obs[7:10]}\")\n",
    "print(f\"\\nActive box: {env.active_box}\")\n",
    "\n",
    "# Test an action\n",
    "action = np.array([0.1, 0, 0, 0, 0, 0, 0, -0.5])  # Move joint 1, close gripper\n",
    "obs_new, reward, terminated, truncated, _ = env.step(action)\n",
    "print(f\"\\nAfter action:\")\n",
    "print(f\"  EE position changed: {obs_new[:3]}\")\n",
    "print(f\"  Gripper width changed: {obs_new[6]:.4f}\")\n",
    "\n",
    "env.close()\n",
    "print(\"\\n✓ Observation format is correct!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_distances():\n",
    "    \"\"\"Analyze minimum distances achieved during episodes.\"\"\"\n",
    "    from stable_baselines3 import PPO\n",
    "    \n",
    "    try:\n",
    "        model = PPO.load(\"touch_box_ppo\")\n",
    "    except:\n",
    "        print(\"Model not found. Train first with: train_pick_place()\")\n",
    "        return\n",
    "    \n",
    "    print(\"Distance Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    min_distances = []\n",
    "    \n",
    "    for ep in range(10):\n",
    "        env = PickPlaceEnv(model_path='model/pick_place_scene.xml')\n",
    "        obs, _ = env.reset()\n",
    "        \n",
    "        ep_min_distance = float('inf')\n",
    "        \n",
    "        while env.step_count < env.max_episode_steps:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, _, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            ee_pos = obs[:3]\n",
    "            box_pos = obs[7:10]\n",
    "            distance = np.linalg.norm(ee_pos - box_pos)\n",
    "            \n",
    "            ep_min_distance = min(ep_min_distance, distance)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        min_distances.append(ep_min_distance)\n",
    "        print(f\"Episode {ep + 1}: min distance = {ep_min_distance:.4f}m\")\n",
    "        env.close()\n",
    "    \n",
    "    min_distances = np.array(min_distances)\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Minimum distance statistics:\")\n",
    "    print(f\"  Mean: {min_distances.mean():.4f}m\")\n",
    "    print(f\"  Std: {min_distances.std():.4f}m\")\n",
    "    print(f\"  Min: {min_distances.min():.4f}m\")\n",
    "    print(f\"  Max: {min_distances.max():.4f}m\")\n",
    "    print(f\"\\nCurrent touch threshold: 0.15m\")\n",
    "    print(f\"Episodes below 0.15m: {(min_distances < 0.15).sum()}/10\")\n",
    "    print(f\"Episodes below 0.20m: {(min_distances < 0.20).sum()}/10\")\n",
    "    print(f\"Episodes below 0.25m: {(min_distances < 0.25).sum()}/10\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "analyze_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4720704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_policy_actions():\n",
    "    \"\"\"Inspect what actions the trained policy produces (for debugging).\"\"\"\n",
    "    from stable_baselines3 import PPO\n",
    "    \n",
    "    try:\n",
    "        model = PPO.load(\"touch_box_ppo\")\n",
    "    except:\n",
    "        print(\"Model not found. Train first with: train_pick_place()\")\n",
    "        return\n",
    "    \n",
    "    env = PickPlaceEnv(model_path='model/pick_place_scene.xml')\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    print(\"Policy Action Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    gripper_actions = []\n",
    "    arm_joint_actions = []\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(100):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, _, _, _ = env.step(action)\n",
    "        \n",
    "        gripper_actions.append(action[7])\n",
    "        arm_joint_actions.append(np.mean(np.abs(action[:7])))  # Mean absolute arm movement\n",
    "        \n",
    "        # Extract distance from observation (obs[0:3] is EE, obs[7:10] is box)\n",
    "        ee_pos = obs[:3]\n",
    "        box_pos = obs[7:10]\n",
    "        distance = np.linalg.norm(ee_pos - box_pos)\n",
    "        distances.append(distance)\n",
    "    \n",
    "    gripper_actions = np.array(gripper_actions)\n",
    "    arm_actions = np.array(arm_joint_actions)\n",
    "    distances = np.array(distances)\n",
    "    \n",
    "    print(\"Gripper Action Statistics:\")\n",
    "    print(f\"  Mean: {gripper_actions.mean():.3f}\")\n",
    "    print(f\"  Std: {gripper_actions.std():.3f}\")\n",
    "    print(f\"  Min: {gripper_actions.min():.3f}\")\n",
    "    print(f\"  Max: {gripper_actions.max():.3f}\")\n",
    "    print(f\"  % closing (< -0.1): {100 * (gripper_actions < -0.1).mean():.1f}%\")\n",
    "    print(f\"  % opening (> +0.1): {100 * (gripper_actions > 0.1).mean():.1f}%\")\n",
    "    print()\n",
    "    print(\"Arm Movement Statistics:\")\n",
    "    print(f\"  Mean absolute movement: {arm_actions.mean():.3f}\")\n",
    "    print(f\"  Max movement: {arm_actions.max():.3f}\")\n",
    "    print()\n",
    "    print(\"Distance to Box:\")\n",
    "    print(f\"  Initial: {distances[0]:.3f}m\")\n",
    "    print(f\"  Final: {distances[-1]:.3f}m\")\n",
    "    print(f\"  Min: {distances.min():.3f}m\")\n",
    "    print(f\"  Mean: {distances.mean():.3f}m\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "inspect_policy_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c57d1",
   "metadata": {},
   "source": [
    "## Visualize the Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the empty scene without training\n",
    "import mujoco.viewer\n",
    "\n",
    "model_path = 'model/pick_place_scene.xml'\n",
    "model = mujoco.MjModel.from_xml_path(model_path)\n",
    "data = mujoco.MjData(model)\n",
    "\n",
    "# Run simulation for a few seconds to see the scene\n",
    "with mujoco.viewer.launch_passive(model, data) as viewer:\n",
    "    # Run for 2 seconds\n",
    "    start = time.time()\n",
    "    while viewer.is_running() and time.time() - start < 10.0:\n",
    "        step_start = data.time\n",
    "        while (data.time - step_start) < model.opt.timestep * 10:\n",
    "            mujoco.mj_step(model, data)\n",
    "        \n",
    "        viewer.sync()\n",
    "        time.sleep(0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
